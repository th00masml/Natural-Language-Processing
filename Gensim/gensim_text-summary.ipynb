{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The goal of Google Research is to work on long-term, ambitious problems, with an emphasis on solving ones that will dramatically help people throughout their daily lives. In pursuit of that goal in 2019, we made advances in a broad set of fundamental research areas, applied our research to new and emerging areas such as healthcare and robotics, open sourced a wide variety of code and continued collaborations with Google product teams to build tools and services that are dramatically more helpful for our users. As we start 2020, it’s useful to take a step back and assess the research work we’ve done over the past year, and also to look forward to what sorts of problems we want to tackle in the upcoming years. In that spirit, this blog post is a survey of some of the research-focused work done by Google researchers and engineers during 2019 (in the spirit of similar reviews for 2018, and more narrowly focused reviews of some work in 2017 and 2016). For a more comprehensive look, please see our research publications in 2019. In 2018, we published a set of AI Principles that provide a framework by which we evaluate our own research and applications of technologies such as machine learning in our products. In June 2019, we published a one-year update about how these principles are being put into practice in many different aspects of our research and product development life cycles. Since many of the areas touched on by the principles are active areas of research in the broader AI and machine learning research community (such as bias, safety, fairness, accountability, transparency and privacy in machine learning systems), our goals are to apply the best currently-known techniques in these areas to our work, and also to do research to continue to advance the state of the art in these important areas. There is enormous potential for machine learning to help with many important societal issues. We have been doing work in several such areas, as well as working to enable others to apply their creativity and skills to solving such problems. Floods are the most common and the most deadly natural disaster on the planet, affecting approximately 250 million people each year. We have been using machine learning, computation and better sources of data to make significantly more accurate flood forecasts, and then to deliver actionable alerts to the phones of millions of people in the affected regions. We also hosted a workshop that brought together researchers with expertise in flood forecasting, hydrology and machine learning from Google and the broader research community to discuss ways to collaborate further on this important problem.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The goal of Google Research is to work on long-term, ambitious problems, with an emphasis on solving ones that will dramatically help people throughout their daily lives. In pursuit of that goal in 2019, we made advances in a broad set of fundamental research areas, applied our research to new and emerging areas such as healthcare and robotics, open sourced a wide variety of code and continued collaborations with Google product teams to build tools and services that are dramatically more helpful for our users. As we start 2020, it’s useful to take a step back and assess the research work we’ve done over the past year, and also to look forward to what sorts of problems we want to tackle in the upcoming years. In that spirit, this blog post is a survey of some of the research-focused work done by Google researchers and engineers during 2019 (in the spirit of similar reviews for 2018, and more narrowly focused reviews of some work in 2017 and 2016). For a more comprehensive look, please see our research publications in 2019. In 2018, we published a set of AI Principles that provide a framework by which we evaluate our own research and applications of technologies such as machine learning in our products. In June 2019, we published a one-year update about how these principles are being put into practice in many different aspects of our research and product development life cycles. Since many of the areas touched on by the principles are active areas of research in the broader AI and machine learning research community (such as bias, safety, fairness, accountability, transparency and privacy in machine learning systems), our goals are to apply the best currently-known techniques in these areas to our work, and also to do research to continue to advance the state of the art in these important areas. There is enormous potential for machine learning to help with many important societal issues. We have been doing work in several such areas, as well as working to enable others to apply their creativity and skills to solving such problems. Floods are the most common and the most deadly natural disaster on the planet, affecting approximately 250 million people each year. We have been using machine learning, computation and better sources of data to make significantly more accurate flood forecasts, and then to deliver actionable alerts to the phones of millions of people in the affected regions. We also hosted a workshop that brought together researchers with expertise in flood forecasting, hydrology and machine learning from Google and the broader research community to discuss ways to collaborate further on this important problem.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This summarizer is based on the , from an “TextRank” algorithm by Mihalcea et al.\n",
    "# This algorithm was later improved upon by Barrios et al., by introducing something called a “BM25 ranking function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint as print\n",
    "from gensim.summarization import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-12 20:08:44,349 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-01-12 20:08:44,350 : INFO : built Dictionary(134 unique tokens: ['ambiti', 'daili', 'dramat', 'emphasi', 'goal']...) from 13 documents (total 211 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('In pursuit of that goal in 2019, we made advances in a broad set of '\n",
      " 'fundamental research areas, applied our research to new and emerging areas '\n",
      " 'such as healthcare and robotics, open sourced a wide variety of code and '\n",
      " 'continued collaborations with Google product teams to build tools and '\n",
      " 'services that are dramatically more helpful for our users.\\n'\n",
      " 'We also hosted a workshop that brought together researchers with expertise '\n",
      " 'in flood forecasting, hydrology and machine learning from Google and the '\n",
      " 'broader research community to discuss ways to collaborate further on this '\n",
      " 'important problem.')\n"
     ]
    }
   ],
   "source": [
    "print(summarize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-12 20:08:44,373 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-01-12 20:08:44,375 : INFO : built Dictionary(134 unique tokens: ['ambiti', 'daili', 'dramat', 'emphasi', 'goal']...) from 13 documents (total 211 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In pursuit of that goal in 2019, we made advances in a broad set of '\n",
      " 'fundamental research areas, applied our research to new and emerging areas '\n",
      " 'such as healthcare and robotics, open sourced a wide variety of code and '\n",
      " 'continued collaborations with Google product teams to build tools and '\n",
      " 'services that are dramatically more helpful for our users.',\n",
      " 'We also hosted a workshop that brought together researchers with expertise '\n",
      " 'in flood forecasting, hydrology and machine learning from Google and the '\n",
      " 'broader research community to discuss ways to collaborate further on this '\n",
      " 'important problem.']\n"
     ]
    }
   ],
   "source": [
    "print(summarize(text, split=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the “ratio” parameter, you specify what fraction of sentences in the original text should be returned as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-12 20:08:44,400 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-01-12 20:08:44,402 : INFO : built Dictionary(134 unique tokens: ['ambiti', 'daili', 'dramat', 'emphasi', 'goal']...) from 13 documents (total 211 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The goal of Google Research is to work on long-term, ambitious problems, '\n",
      " 'with an emphasis on solving ones that will dramatically help people '\n",
      " 'throughout their daily lives.\\n'\n",
      " 'In pursuit of that goal in 2019, we made advances in a broad set of '\n",
      " 'fundamental research areas, applied our research to new and emerging areas '\n",
      " 'such as healthcare and robotics, open sourced a wide variety of code and '\n",
      " 'continued collaborations with Google product teams to build tools and '\n",
      " 'services that are dramatically more helpful for our users.\\n'\n",
      " 'In 2018, we published a set of AI Principles that provide a framework by '\n",
      " 'which we evaluate our own research and applications of technologies such as '\n",
      " 'machine learning in our products.\\n'\n",
      " 'Since many of the areas touched on by the principles are active areas of '\n",
      " 'research in the broader AI and machine learning research community (such as '\n",
      " 'bias, safety, fairness, accountability, transparency and privacy in machine '\n",
      " 'learning systems), our goals are to apply the best currently-known '\n",
      " 'techniques in these areas to our work, and also to do research to continue '\n",
      " 'to advance the state of the art in these important areas.\\n'\n",
      " 'We have been using machine learning, computation and better sources of data '\n",
      " 'to make significantly more accurate flood forecasts, and then to deliver '\n",
      " 'actionable alerts to the phones of millions of people in the affected '\n",
      " 'regions.\\n'\n",
      " 'We also hosted a workshop that brought together researchers with expertise '\n",
      " 'in flood forecasting, hydrology and machine learning from Google and the '\n",
      " 'broader research community to discuss ways to collaborate further on this '\n",
      " 'important problem.')\n"
     ]
    }
   ],
   "source": [
    "print(summarize(text, ratio=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the “word_count” parameter, we specify the maximum amount of words we want in the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-12 20:08:44,430 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-01-12 20:08:44,431 : INFO : built Dictionary(134 unique tokens: ['ambiti', 'daili', 'dramat', 'emphasi', 'goal']...) from 13 documents (total 211 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('In pursuit of that goal in 2019, we made advances in a broad set of '\n",
      " 'fundamental research areas, applied our research to new and emerging areas '\n",
      " 'such as healthcare and robotics, open sourced a wide variety of code and '\n",
      " 'continued collaborations with Google product teams to build tools and '\n",
      " 'services that are dramatically more helpful for our users.')\n"
     ]
    }
   ],
   "source": [
    "print(summarize(text, word_count=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('research\\n'\n",
      " 'researchers\\n'\n",
      " 'year\\n'\n",
      " 'years\\n'\n",
      " 'product\\n'\n",
      " 'products\\n'\n",
      " 'floods\\n'\n",
      " 'flood\\n'\n",
      " 'people\\n'\n",
      " 'natural\\n'\n",
      " 'affecting\\n'\n",
      " 'affected\\n'\n",
      " 'areas\\n'\n",
      " 'problems\\n'\n",
      " 'problem\\n'\n",
      " 'important\\n'\n",
      " 'safety\\n'\n",
      " 'accountability')\n"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import keywords\n",
    "print(keywords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try on bigger text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'433\\nVOL. L'\n"
     ]
    }
   ],
   "source": [
    "with open(r'C:\\Users\\bnawa\\turing_imitation-game.txt', 'r') as f2:\n",
    "    data = f2.read()\n",
    "    print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-12 20:16:46,738 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-01-12 20:16:46,750 : INFO : built Dictionary(1457 unique tokens: ['vol', 'lix', 'octob', 'quarterli', 'review']...) from 1450 documents (total 5063 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I PROPOSE to consider the question, ‘Can machines think?’ This should\\n'\n",
      " 'We now ask the question, ‘What will happen when a machine takes\\n'\n",
      " 'means to put the appropriate instruction table into the machine so that it\\n'\n",
      " 'This example is typical of discrete state machines.\\n'\n",
      " 'It will seem that given the initial state of the machine and the input\\n'\n",
      " 'But the number of states of which such a machine\\n'\n",
      " 'the number of states of three Manchester machines put together.\\n'\n",
      " 'Given the table corresponding to a discrete state machine it is possible\\n'\n",
      " 'behaviour of any discrete state machine.\\n'\n",
      " 'played with the machine in question (as B) and the mimicking digital\\n'\n",
      " 'discrete state machine, is described by saying that they are universal\\n'\n",
      " 'various new machines to do various computing processes.\\n'\n",
      " 'suggested tentatively that the question, ‘Can machines think?’ should be\\n'\n",
      " 'general and ask ‘Are there discrete state machines which would do well?’\\n'\n",
      " 'ready to proceed to the debate on our question, ‘Can machines think?’ and\\n'\n",
      " 'The original question, ‘Can machines think!’ I believe to be\\n'\n",
      " 'altered so much that one will be able to speak of machines thinking without\\n'\n",
      " 'argument of exactly similar form may be made for the case of machines.\\n'\n",
      " 'the powers of discrete-state machines.\\n'\n",
      " 'The result in question refers to a type of machine which\\n'\n",
      " 'machine was able to answer like this in the viva voce?\\n'\n",
      " 'the interrogator could distinguish the machine from the man simply by\\n'\n",
      " 'machine (programmed for playing the game) would not attempt to give the\\n'\n",
      " 'machine can do one of these things, and describes the kind of method that\\n'\n",
      " 'some discrete-state machine has the property.\\n'\n",
      " 'It is true that a discrete-state machine must be different from a\\n'\n",
      " 'differential analyser is a certain kind of machine not of the '\n",
      " 'discrete-state\\n'\n",
      " 'Then given a discrete-state machine it should certainly be possible\\n'\n",
      " 'to programme these machines to play the game.')\n"
     ]
    }
   ],
   "source": [
    "print(summarize(data, ratio=0.02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('machines\\n'\n",
      " 'machine\\n'\n",
      " 'number\\n'\n",
      " 'numbers\\n'\n",
      " 'state\\n'\n",
      " 'states\\n'\n",
      " 'stated\\n'\n",
      " 'certain\\n'\n",
      " 'digital\\n'\n",
      " 'digits\\n'\n",
      " 'computing\\n'\n",
      " 'computers\\n'\n",
      " 'computations\\n'\n",
      " 'computation\\n'\n",
      " 'computable\\n'\n",
      " 'time\\n'\n",
      " 'times\\n'\n",
      " 'man\\n'\n",
      " 'form\\n'\n",
      " 'forms\\n'\n",
      " 'forming\\n'\n",
      " 'argument\\n'\n",
      " 'arguments\\n'\n",
      " 'human\\n'\n",
      " 'answer\\n'\n",
      " 'answers\\n'\n",
      " 'answered')\n"
     ]
    }
   ],
   "source": [
    "print(keywords(data, ratio=0.02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Montemurro and Zanette’s entropy based keyword extraction algorithm\n",
    "# identify words that play a significant role in the large-scale structure of a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import mz_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 0.014861524445267834),\n",
      " ('to', 0.007767813733759304),\n",
      " ('of', 0.007746836544575982),\n",
      " ('a', 0.007642469804406734),\n",
      " ('in', 0.00491688390579003),\n",
      " ('is', 0.004873761611889092),\n",
      " ('be', 0.004746023441471577),\n",
      " ('that', 0.004656688509147202),\n",
      " ('it', 0.004414697682557859),\n",
      " ('computers', 0.003700867781298998)]\n"
     ]
    }
   ],
   "source": [
    "print(mz_keywords(data,scores=True,threshold=0.001)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('laws', 3.0072808371918014),\n",
      " ('conduct', 2.350930600530324),\n",
      " ('soul', 2.350930600530324),\n",
      " ('child', 2.295789851187828),\n",
      " ('punishments', 2.1465665962663456),\n",
      " ('rewards', 2.1465665962663456),\n",
      " ('souls', 2.1465665962663456),\n",
      " ('thinks', 2.1465665962663456),\n",
      " ('winter', 2.1465665962663456),\n",
      " ('instruction', 1.99605438170079)]\n"
     ]
    }
   ],
   "source": [
    "print(mz_keywords(data,scores=True,weighted=False,threshold=1.0)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
